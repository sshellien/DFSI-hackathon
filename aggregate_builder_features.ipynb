{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "from scipy import stats\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "HACKATHON_DATA_PATH = \"C:\\Hackathon_Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (15,54,57,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "b'Skipping line 197673: expected 109 fields, saw 112\\n'\n",
      "b'Skipping line 365536: expected 109 fields, saw 110\\n'\n",
      "b'Skipping line 414650: expected 109 fields, saw 111\\n'\n",
      "b'Skipping line 424887: expected 109 fields, saw 111\\nSkipping line 424888: expected 109 fields, saw 111\\n'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (0,2,3,4,5,6,9,10,11,13,17,39,45,48,49,67,68,73,74,76,77,82,86,87,89,90,91,92,93,94,95,96,98,100,101,102,103,104,106,107,108) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "complaints_df = pd.read_csv('C:\\Hackathon_Data\\OFT Data files redacted\\OFT Data files redacted\\Fair Trading builder complaints data.csv', encoding='ISO-8859-1', error_bad_lines=False)\n",
    "builder_df = pd.read_csv('C:\\Hackathon_Data\\PII_Redacted_Extract_Files_20181203\\R_BUILDER_EXPORT.txt', encoding='ISO-8859-1', error_bad_lines=False)\n",
    "policy_df = pd.read_csv('C:\\Hackathon_Data\\PII_Redacted_Extract_Files_20181203\\R_POLICY_EXPORT.txt', encoding='ISO-8859-1', error_bad_lines=False)\n",
    "elig_df = pd.read_csv('C:\\Hackathon_Data\\PII_Redacted_Extract_Files_20181203\\R_ELIGIBILITY_EXPORT.txt', encoding='ISO-8859-1', error_bad_lines=False)\n",
    "\n",
    "complaints_df['Licence'] = complaints_df['Licence'].str.lstrip()\n",
    "complaints_df = complaints_df[complaints_df['Licence'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging tables...\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "elig_df = elig_df[elig_df['current_elig'].notna()]\n",
    "elig_df = elig_df[elig_df['current_elig'].str.contains('Y')]\n",
    "\n",
    "print('Merging tables...')\n",
    "complaints_builder_df = complaints_df.merge(builder_df, left_on='Licence', right_on='builder_bk', how='left')\n",
    "policy_builder_df = policy_df.merge(builder_df, left_on='builder_bk', right_on='builder_bk', how='left')\n",
    "builder_df = elig_df.merge(builder_df, left_on='builder_licence_number', right_on='builder_bk', how='left')\n",
    "\n",
    "claims_df = pd.read_csv('C:\\Hackathon_Data\\PII_Redacted_Extract_Files_20181203\\R_CLAIM_EXPORT.txt', encoding = \"ISO-8859-1\", error_bad_lines=False )\n",
    "policy_with_claim = list(claims_df['policy_bk'].values)\n",
    "policy_builder_df['Has_Claim'] = policy_builder_df['policy_bk'].isin(policy_with_claim).astype(int)\n",
    "\n",
    "print(policy_builder_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Converting Date Formats')\n",
    "from pprint import pprint\n",
    "\n",
    "def parse_policy_dates(series):\n",
    "    series = series.astype(str).replace('\\.0', '', regex=True)\n",
    "    return pd.to_datetime(series, format='%Y%m%d', errors='coerce')\n",
    "\n",
    "def get_numeric_stats(series):\n",
    "    mean = np.mean(series[series.notna()])\n",
    "    std = np.std(series[series.notna()])\n",
    "    skew = stats.skew(series[series.notna()])\n",
    "    kurtosis = stats.kurtosis(series[series.notna()])\n",
    "#     print('{}: mean:{:.2f} std:{:.2f}'.format(series.name, mean, std))\n",
    "    return mean, std, skew, kurtosis\n",
    "\n",
    "def unpack_value_counts(series, prefix=''):\n",
    "    counts = series.value_counts(normalize=True, sort=True, ascending=False, bins=None, dropna=True)\n",
    "    \n",
    "    out_dict = {}\n",
    "    for k, v in counts.to_dict().items():\n",
    "        new_key = '{}{}_{}_proportion'.format(prefix, series.name, k).lower().replace(' ', '_').replace('\\\\', '_')\n",
    "        out_dict[new_key] = v\n",
    "    return out_dict\n",
    "\n",
    "def normalize(df, columns_to_normalize):\n",
    "    result = df.copy()\n",
    "    for feature_name in columns_to_normalize:\n",
    "        max_value = df[feature_name].max()\n",
    "        min_value = df[feature_name].min()\n",
    "        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result\n",
    "    \n",
    "policy_builder_df['issue_date_parsed'] = parse_policy_dates(policy_builder_df['issue_date'])\n",
    "policy_builder_df['act_completion_date_parsed'] = parse_policy_dates(policy_builder_df['act_completion_date'])\n",
    "policy_builder_df['est_completion_date_parsed'] = parse_policy_dates(policy_builder_df['est_completion_date'])\n",
    "\n",
    "policy_builder_df['est_project_duration'] = (policy_builder_df['est_completion_date_parsed'] - policy_builder_df['issue_date_parsed']).dt.days\n",
    "policy_builder_df['act_project_duration'] = (policy_builder_df['act_completion_date_parsed'] - policy_builder_df['issue_date_parsed']).dt.days\n",
    "\n",
    "# List of dictionaries\n",
    "builders_data = []\n",
    "builders_complaints_temporal = []\n",
    "\n",
    "NUM_BUILDERS_TO_PROCESS = 500\n",
    "# NUM_BUILDERS_TO_PROCESS = -1\n",
    "\n",
    "num_total = len(policy_builder_df['builder_bk'][policy_builder_df['builder_bk'].notna()].unique())\n",
    "\n",
    "for count, unique_id in enumerate(policy_builder_df['builder_bk'][policy_builder_df['builder_bk'].notna()].unique()[:NUM_BUILDERS_TO_PROCESS]):\n",
    "    print('Aggregating complaints and policy data for builder {}/{}'.format(count, num_total))\n",
    "    builder_data = {'builder_bk': unique_id}\n",
    "        \n",
    "    # Get info for this builder\n",
    "    masked_builder_df = policy_builder_df[policy_builder_df['builder_bk'] == unique_id]\n",
    "    \n",
    "    builder_categorical_cols = ['builder_entity_type',\n",
    "                                'builder_review_size',\n",
    "                                'builder_review_type',\n",
    "                                'primary_builder_segment_code',\n",
    "                                'secondary_builder_segment_code']\n",
    "    \n",
    "#     display(masked_builder_df)\n",
    "    for colname in builder_categorical_cols:\n",
    "        builder_data[colname] = masked_builder_df[colname].iloc[0]\n",
    "    \n",
    "    # Aggregate the policy data for this builder\n",
    "    masked_policy_builder_df = policy_builder_df[policy_builder_df['builder_bk'] == unique_id]\n",
    "    builder_data['Has_Claim'] = masked_policy_builder_df['Has_Claim'].any()\n",
    "    \n",
    "    # Numeric policy Data\n",
    "    numeric_cols = ['est_project_duration',\n",
    "                    'act_project_duration',\n",
    "                    'contract_amount',\n",
    "                    'approvedjobnumberlimit',\n",
    "                    'approvedjobnumbervalue']\n",
    "   \n",
    "    for colname in numeric_cols:\n",
    "        mean, std, skew, kurtosis = get_numeric_stats(masked_policy_builder_df[colname])\n",
    "        builder_data['mean_{}'.format(colname)] = mean\n",
    "        builder_data['std_{}'.format(colname)] = std\n",
    "        builder_data['skew_{}'.format(colname)] = skew\n",
    "        builder_data['kurtosis_{}'.format(colname)] = kurtosis\n",
    "    \n",
    "    # Categorical policy Data\n",
    "    categorical_cols = ['cover_type',\n",
    "#         'security_type',\n",
    "        'relationship_to_builder',\n",
    "        'speculative_project',\n",
    "        'architectordesigner_tendered',\n",
    "        'builder_licence_status_code']\n",
    "    \n",
    "    for colname in categorical_cols:\n",
    "        builder_data.update(unpack_value_counts(masked_policy_builder_df[colname]))\n",
    "            \n",
    "\n",
    "    # Aggregate the complaint data for this builder\n",
    "    masked_complaint_builder_df = complaints_builder_df[complaints_builder_df['builder_bk'] == unique_id]\n",
    "    masked_complaint_builder_df['Date'] = pd.to_datetime(masked_complaint_builder_df['Registered Date'], format='%d/%m/%y %H:%M')\n",
    "    masked_complaint_builder_df = masked_complaint_builder_df[['Goods', 'Product', 'Practice', 'Date', 'Customer PC']].sort_values(by='Date')\n",
    "    \n",
    "    complaint_counts = normalize(masked_complaint_builder_df[['Date', 'Practice']].groupby(pd.Grouper(key='Date', freq='M')).count(), ['Practice'])\n",
    "    builder_complaints_temporal = complaint_counts.transpose()\n",
    "#     print(unpack_value_counts(complaint_counts))\n",
    "#     try:\n",
    "#         ax = complaint_counts.plot(legend=False)\n",
    "#         ax.set_ylabel('Normalised number of complaints')\n",
    "#         ax.set_title('Temporal histogram of complaints for {}'.format(unique_id))\n",
    "#     except TypeError:\n",
    "#         print('No complaints for this builder')\n",
    "\n",
    "    # Categorical complaint data\n",
    "    builder_data.update(unpack_value_counts(masked_complaint_builder_df['Practice'], prefix='complaint_'))\n",
    "    builder_data.update(unpack_value_counts(masked_complaint_builder_df['Product'], prefix='complaint_'))\n",
    "            \n",
    "    builders_data.append(pd.DataFrame([builder_data], columns=builder_data.keys()))\n",
    "    builders_complaints_temporal.append(builder_complaints_temporal)\n",
    "    \n",
    "builder_features_df = pd.concat(builders_data, axis=0).reset_index()\n",
    "builder_temporal_features_df = pd.concat(builders_complaints_temporal, axis=0).reset_index().fillna(0.0)\n",
    "builder_temporal_features_df = builder_temporal_features_df.add_prefix('complaints_')\n",
    "\n",
    "print(builder_temporal_features_df)\n",
    "                                                      \n",
    "# Fill NaN proportions with zero!\n",
    "cols_to_fill = [col for col in builder_features_df if col.endswith('_proportion')]\n",
    "builder_features_df[cols_to_fill] = builder_features_df[cols_to_fill].fillna(0.0)\n",
    "\n",
    "# display(builder_features_df)\n",
    "print('Saving PKL')\n",
    "builder_features_df.to_pickle('builder_features_df_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "builder_categorical_cols = ['builder_entity_type',\n",
    "                            'builder_review_size',\n",
    "                            'builder_review_type',\n",
    "                            'primary_builder_segment_code',\n",
    "                            'secondary_builder_segment_code']\n",
    "\n",
    "builder_features_encoded_df = builder_features_df\n",
    "\n",
    "for col in builder_categorical_cols:\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    builder_features_encoded_df[col] = le.fit_transform(builder_features_df[col].fillna(''))\n",
    "\n",
    "y = builder_features_encoded_df['Has_Claim']\n",
    "X = builder_features_encoded_df.drop(['builder_bk', 'Has_Claim'], axis=1)\n",
    "\n",
    "X = X.apply(lambda x: x.fillna(x.mean())) \n",
    "\n",
    "display(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# param_grid = {'C':[1,10,100,1000],'gamma':[1,0.1,0.001,0.0001], 'kernel':['rbf']}\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "# grid = GridSearchCV(SVC(),param_grid,refit = True, verbose=2, n_jobs=-1, scoring='f1')\n",
    "# # estimator = RandomForestClassifier()\n",
    "# # selector = RFE(estimator, 25, step=1)\n",
    "# print('Fitting!')\n",
    "# # selector = selector.fit(X, y)\n",
    "# grid.fit(X, y)\n",
    "# pprint(grid.grid_scores_)\n",
    "# print('Done SVC!')\n",
    "\n",
    "# # Number of trees in random forest\n",
    "# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 4)]\n",
    "# # Number of features to consider at every split\n",
    "# max_features = ['sqrt']\n",
    "# # Maximum number of levels in tree\n",
    "# max_depth = [int(x) for x in np.linspace(10, 110, num = 3)]\n",
    "# max_depth.append(None)\n",
    "# # Minimum number of samples required to split a node\n",
    "# min_samples_split = [2, 5, 10]\n",
    "# # Minimum number of samples required at each leaf node\n",
    "# min_samples_leaf = [1, 2, 4]\n",
    "# # Create the random grid\n",
    "# random_grid = {'n_estimators': n_estimators,\n",
    "#                'max_features': max_features,\n",
    "#                'max_depth': max_depth,\n",
    "#                'min_samples_split': min_samples_split,\n",
    "#                'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "# grid = GridSearchCV(RandomForestClassifier(), random_grid, refit = True, verbose=2, n_jobs=-1, scoring='f1')\n",
    "# estimator = RandomForestClassifier(max_depth=60, max_features='sqrt', min_samples_leaf=1, min_samples_split=5, n_estimators=2000)\n",
    "estimator = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "# estimator = KNeighborsClassifier(n_neighbors=5, algorithm='auto')\n",
    "# estimator = QuadraticDiscriminantAnalysis()\n",
    "# estimator = RFE(estimator, 25, step=1)\n",
    "print('Fitting!')\n",
    "# selector = selector.fit(X, y)\n",
    "estimator.fit(X_train, y_train)\n",
    "y_pred = estimator.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f1_score(y_test, y_pred, average='binary'))\n",
    "print('micro', f1_score(y_test, y_pred, average='micro'))\n",
    "print('macro', f1_score(y_test, y_pred, average='macro'))\n",
    "print('weighted', f1_score(y_test, y_pred, average='weighted'))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(8,4))\n",
    "# ax1 = sb.countplot(x='builder_entity_type', data=builder_df)\n",
    "# plt.figure(figsize=(20,4))\n",
    "# ax2 = sb.countplot(x='builder_review_size', data=builder_df)\n",
    "# plt.figure(figsize=(8,4))\n",
    "# ax3 = sb.countplot(x='builder_review_type', data=builder_df)\n",
    "# plt.figure(figsize=(20,4))\n",
    "# ax4 = sb.countplot(x='primary_builder_segment_code', data=builder_df)\n",
    "# plt.figure(figsize=(20,4))\n",
    "# ax5 = sb.countplot(x='secondary_builder_segment_code', data=builder_df)\n",
    "# plt.figure(figsize=(8,4))\n",
    "# ax = sb.countplot(x='cover_type', data=policy_builder_df)\n",
    "# plt.figure(figsize=(8,4))\n",
    "# ax = sb.countplot(x='security_type', data=policy_builder_df)\n",
    "# plt.figure(figsize=(8,4))\n",
    "# ax = sb.countplot(x='relationship_to_builder', data=policy_builder_df)\n",
    "# plt.figure(figsize=(8,4))\n",
    "# ax = sb.countplot(x='speculative_project', data=policy_builder_df)\n",
    "# plt.figure(figsize=(8,4))\n",
    "# ax = sb.countplot(x='architectordesigner_tendered', data=policy_builder_df)\n",
    "# plt.figure(figsize=(8,4))\n",
    "# ax = sb.countplot(x='builder_licence_status_code', data=policy_builder_df)\n",
    "\n",
    "# for i in range(1,9):\n",
    "#     plt.figure(figsize=(8,4))\n",
    "#     ax = sb.countplot(x='c0{}_cover_yn'.format(i), data=builder_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
